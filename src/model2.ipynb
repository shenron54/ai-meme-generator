{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction with RAKE Algorithm\n",
    "\n",
    "To extract keywords from text using the RAKE (Rapid Automatic Keyword Extraction) algorithm, we utilize the `rake_nltk` library. First, we initialize an instance of `Rake`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "rake = Rake()\n",
    "\n",
    "def get_keywords(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: Splitting Data into Train and Validation Sets\n",
    "\n",
    "To organize our data for training and validation, we create lists `train_data` and `val_data` to store captions and labels based on their respective data types ('train' or 'val').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 719455 [('Y U No', 'commercial <sep> y u no same volume as show!?'), ('Y U No', 'Victoria <sep> y u no tell us your secret?!'), ('Y U No', 'KONY <sep> Y u no take justin bieber'), ('Y U No', 'TED <sep> y u no tell us how you met their mother'), ('Y U No', 'universal remote <sep> y u no work on universe?')]\n",
      "Val data length: 179864 [('Y U No', 'Google <sep> Y U NO LET ME FINISH TYPING?'), ('Y U No', 'i held the door <sep> y u no say thank you'), ('Y U No', 'Team rocket <sep> y u no catch a different pikachu?'), ('Y U No', 'Y u no guy <sep> y u sound asian in my head?'), ('Y U No', 'hands <sep> y u no have same amount of fingers?')]\n"
     ]
    }
   ],
   "source": [
    "# Create lists to store captions and labels\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    template = row['Template']\n",
    "    caption = row['Caption']\n",
    "    data_type = row['data_type']\n",
    "    \n",
    "    # Append the template and caption data to the appropriate list based on data_type\n",
    "    if data_type == 'train':\n",
    "        train_data.append((template, caption))\n",
    "    elif data_type == 'val':\n",
    "        val_data.append((template, caption))\n",
    "\n",
    "# Print the length of the train and val data\n",
    "print(\"Train data length:\", len(train_data), train_data[:5])\n",
    "print(\"Val data length:\", len(val_data), val_data[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 719455 719455 719455\n",
      "Val data length: 179864 179864 179864\n",
      "The top 10 strings with the largest number of words have 266 words each:\n",
      "1. cuz bryan can'T go so he dont need to be in this convo <sep> good\n",
      "Grumpy cat good\t0\tBlake's moving away? <sep> Good.\n",
      "Grumpy cat good\t0\tDid you forget To pack something? <sep> Good.\n",
      "Grumpy cat good\t0\tsocial media? <sep> No.\n",
      "Grumpy cat good\t0\tmaisy got hit by a car? <sep> good\n",
      "Grumpy cat good\t0\tclayton has down syndrome? <sep> good\n",
      "Grumpy cat good\t0\tyou lost your job? <sep> good\n",
      "Grumpy cat good\t0\tyou can't find a job number? <sep> good\n",
      "Grumpy cat good\t0\tyour hours are cut? <sep> good\n",
      "Grumpy cat good\t0\teveryone hates your squeaky shoes? <sep> good\n",
      "Grumpy cat good\t0\tyou have a goitre? <sep> good\n",
      "Grumpy cat good\t0\tWes left his wallet at home? <sep> good\n",
      "Grumpy cat good\t0\tbubbler for lunch wes? <sep> good\n",
      "Grumpy cat good\t0\tyou hate tom waits? <sep> good\n",
      "Grumpy cat good\t0\tOU LOST? <sep> GOOD\n",
      "Grumpy cat good\t0\tYOu didn't get a deal? <sep> Good\n",
      "Grumpy cat good\t0\tUnpause your fraudulent pub? <sep> no.\n",
      "Grumpy cat good\t0\tRoxy Doesn't Like Cats <sep> Good!\n",
      "Grumpy cat good\t0\tndak berjaya apply cuti? <sep> good\n",
      "Grumpy cat good\t0\tmasih kerja hari ni? <sep> good\n",
      "Grumpy cat good\t0\tHappy Pi day! <sep> How about a pi in your face\n",
      "Grumpy cat good\t0\tBeyonce not coming to new zealand? <sep> good\n",
      "Grumpy cat good\t0\tI Gave you Bartonella <sep> Good\n",
      "Grumpy cat good\t0\tTHEY said to me i hate you\" <sep> well, I'm glad,we have something in common\n"
     ]
    }
   ],
   "source": [
    "# Data inspection \n",
    "train_captions = []\n",
    "train_classes = []\n",
    "train_keywords = []\n",
    "\n",
    "val_captions = []\n",
    "val_classes = []\n",
    "val_keywords = []\n",
    "\n",
    "# Iterate over the train_data list\n",
    "for template, caption in train_data:\n",
    "    train_captions.append(caption)\n",
    "    train_classes.append(template)\n",
    "    kw = get_keywords(caption)\n",
    "    train_keywords.append(kw)\n",
    "\n",
    "# Iterate over the val_data list\n",
    "for template, caption in val_data:\n",
    "    val_captions.append(caption)\n",
    "    val_classes.append(template)\n",
    "    kw = get_keywords(caption)\n",
    "    val_keywords.append(kw)\n",
    "\n",
    "# Print the length of the train and val data\n",
    "print(\"Train data length:\", len(train_captions), len(train_classes), len(train_keywords))\n",
    "print(\"Val data length:\", len(val_captions), len(val_classes), len(val_keywords))\n",
    "\n",
    "\n",
    "all_captions = train_captions + val_captions    \n",
    "largest_word_count = 0\n",
    "longest_strings = []\n",
    "smallest_word_count = float('inf')\n",
    "shortest_string = \"\"\n",
    "\n",
    "for string in all_captions:\n",
    "    word_count = len(string.split())\n",
    "    if word_count > largest_word_count:\n",
    "        largest_word_count = word_count\n",
    "        longest_strings = [string]\n",
    "    elif word_count == largest_word_count:\n",
    "        longest_strings.append(string)\n",
    "\n",
    "# Sort the longest strings by length\n",
    "longest_strings.sort(key=len, reverse=True)\n",
    "\n",
    "# Print the top 10 longest strings\n",
    "print(f\"The top 10 strings with the largest number of words have {largest_word_count} words each:\")\n",
    "for i, string in enumerate(longest_strings[:10]):\n",
    "    print(f\"{i+1}. {string}\")\n",
    "\n",
    "# print(f\"\\nThe string with the smallest number of words has {smallest_word_count} words:\")\n",
    "# print(shortest_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Tokens and Unfreezing Strategy for Text Encoding\n",
    "\n",
    "In this section, we define special tokens and specify a strategy for unfreezing layers during text encoding.\n",
    "\n",
    "We declare a dictionary named `SPECIAL_TOKENS` containing special tokens used for encoding input captions. These tokens include:\n",
    "- `bos_token`: Beginning of sequence token\n",
    "- `eos_token`: End of sequence token\n",
    "- `unk_token`: Unknown token\n",
    "- `pad_token`: Padding token\n",
    "- `sep_token`: Separator token\n",
    "\n",
    "These special tokens are essential for effectively training the model to conditionally generate text, ensuring proper sequence generation and decoding.\n",
    "\n",
    "Additionally, we set a parameter `UNFREEZE_LAST_N` to `6`, representing the number of layers to unfreeze during the model training process. This strategy allows for fine-tuning specific layers within the text encoder, balancing between model performance and computational efficiency.\n",
    "\n",
    "The combination of special tokens and unfreezing strategy contributes to the successful training and generation of text sequences using the specified model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG           = False\n",
    "\n",
    "INPUT_DIR       = 'articles'\n",
    "\n",
    "USE_APEX        = True\n",
    "APEX_OPT_LEVEL  = 'O1'\n",
    "\n",
    "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
    "\n",
    "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
    "\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",\n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "\n",
    "MAXLEN          = 128  #{768, 1024, 1280, 1600}\n",
    "\n",
    "TRAIN_SIZE      = 0.8\n",
    "\n",
    "if USE_APEX:\n",
    "    TRAIN_BATCHSIZE = 4\n",
    "    BATCH_UPDATE    = 16\n",
    "else:\n",
    "    TRAIN_BATCHSIZE = 2\n",
    "    BATCH_UPDATE    = 32\n",
    "\n",
    "EPOCHS          = 4\n",
    "LR              = 5e-4\n",
    "EPS             = 1e-8\n",
    "WARMUP_STEPS    = 1e2\n",
    "\n",
    "SEED            = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Caption Dataset for Text Processing\n",
    "\n",
    "To facilitate data loading and preprocessing for caption-based tasks, we implement a custom dataset class `CaptionDataset` inheriting from `torch.utils.data.Dataset`.\n",
    "\n",
    "The `CaptionDataset` class is designed to handle input data in the form of tuples containing template, caption, and keywords. Here's a breakdown of its key functionalities:\n",
    "\n",
    "- **Initialization (`__init__`):**\n",
    "  - Initializes the dataset by extracting template, caption, and keywords from the input `data`.\n",
    "  - Sets attributes such as `randomize` (for randomization during keyword processing) and `tokenizer` (for text tokenization).\n",
    "  \n",
    "- **Static Method (`join_keywords`):**\n",
    "  - Combines and formats keywords into a single string, optionally randomizing the keyword order if `randomize` is `True`.\n",
    "  \n",
    "- **Length Method (`__len__`):**\n",
    "  - Returns the total number of caption samples in the dataset.\n",
    "\n",
    "- **Get Item Method (`__getitem__`):**\n",
    "  - Retrieves a specific caption sample (`i`) from the dataset.\n",
    "  - Constructs an input sequence by concatenating template, keywords, and caption using special tokens (`SPECIAL_TOKENS`).\n",
    "  - Tokenizes the input sequence using the specified `tokenizer`, ensuring truncation and padding to a maximum length (`MAX_LENGTH`).\n",
    "  - Returns a dictionary containing:\n",
    "    - `\"label\"`: Tensor representing the input sequence (`input_ids`).\n",
    "    - `\"input_ids\"`: Tensor representing the input sequence (`input_ids`).\n",
    "    - `\"attention_mask\"`: Tensor representing the attention mask for the input sequence.\n",
    "\n",
    "This `CaptionDataset` class encapsulates the data preprocessing pipeline, allowing seamless integration with PyTorch's data load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom caption dataset \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, randomize=False):\n",
    "        template, caption, keywords  = [], [], []\n",
    "        for temp, cap, kws in data:\n",
    "            template.append(temp)\n",
    "            caption.append(cap)\n",
    "            keywords.append(kws)\n",
    "        \n",
    "        self.randomize = randomize    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.template = template\n",
    "        self.keywords = keywords\n",
    "        self.caption = caption\n",
    "    \n",
    "    @staticmethod\n",
    "    def join_keywords(keywords, randomize=False):\n",
    "        N = len(keywords)\n",
    "\n",
    "        #random sampling and shuffle\n",
    "        if randomize: \n",
    "            M = random.choice(range(N+1))\n",
    "            keywords = keywords[:M]\n",
    "            random.shuffle(keywords)\n",
    "\n",
    "        return ','.join(keywords)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        keywords = self.keywords[i].copy()\n",
    "        kw = self.join_keywords(keywords)\n",
    "        input = SPECIAL_TOKENS['bos_token'] + self.template[i] + \\\n",
    "                SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
    "                self.caption[i] + SPECIAL_TOKENS['eos_token']\n",
    "        \n",
    "        encodings_dict = self.tokenizer(input,\n",
    "                                   truncation = True,\n",
    "                                   max_length = MAX_LENGTH,\n",
    "                                   padding=\"max_length\")\n",
    "        \n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            \"label\": torch.tensor(input_ids),\n",
    "            \"input_ids\": torch.tensor(input_ids),            \n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Language Model and Tokenizer Initialization\n",
    "\n",
    "To initialize a GPT-2 language model and tokenizer for text generation tasks, we define two utility functions: `get_tokenizer` and `get_model`.\n",
    "\n",
    "## `get_tokenizer` Function\n",
    "\n",
    "The `get_tokenizer` function retrieves a GPT-2 tokenizer (`GPT2TokenizerFast`) from the `\"openai-community/gpt2\"` pretrained model. It supports adding custom special tokens if provided. The function signature is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "\n",
    "def get_tokenier(special_tokens=None):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\") #GPT2Tokenizer\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "\n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = GPT2Config.from_pretrained(\"gpt2\", \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = GPT2Config.from_pretrained(\"gpt2\",                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.from_pretrained(load_model_path)\n",
    "\n",
    "    model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing and Unfreezing Model Layers for Fine-Tuning\n",
    "\n",
    "The provided code snippet demonstrates a strategy for selectively freezing and unfreezing model layers during fine-tuning of a GPT-2 model.\n",
    "\n",
    "- **Freezing All Parameters:**\n",
    "  - Initially, all model parameters are set to `requires_grad = False`, effectively freezing the entire model.\n",
    "\n",
    "- **Unfreezing Last n Transformer Blocks:**\n",
    "  - The code selectively unfreezes the last `n` transformer blocks (`n = 6` in this case) by setting their parameters to `requires_grad = True`.\n",
    "\n",
    "- **Unfreezing Specific Layers:**\n",
    "  - Additionally, specific layers such as layer normalization (`ln_f`) and the language model head (`lm_head`) are unfrozen by setting their parameters to `requires_grad = True`.\n",
    "\n",
    "This fine-grained control over parameter updates allows for targeted fine-tuning of specific model components while keeping others fixed. It optimizes training efficiency and facilitates adaptation of the model to new tasks or datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    #Only un-freeze the last n transformer blocks\n",
    "    if i >= 6:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading with DataLoader\n",
    "\n",
    "The provided code snippet demonstrates the setup of data loaders (`DataLoader`) for training and validation datasets (`train_dataset` and `val_dataset`) using the `torch.utils.data` module.\n",
    "\n",
    "- **Training DataLoader (`train_dataloader`):**\n",
    "  - Utilizes `RandomSampler` for random sampling of training data (`train_dataset`).\n",
    "  - Specifies the batch size (`BATCH_UPDATE`) for batching input data during training.\n",
    "\n",
    "- **Validation DataLoader (`val_dataloader`):**\n",
    "  - Similarly uses `RandomSampler` for random sampling of validation data (`val_dataset`).\n",
    "  - Configures the batch size (`BATCH_UPDATE`) for processing validation data in batches.\n",
    "\n",
    "These data loaders facilitate efficient data loading and batching for model training and evaluation, ensuring randomized sampling and batched processing of caption datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_dataset = CaptionDataset(train_data, tokenizer=tokenizer)\n",
    "val_dataset = CaptionDataset(val_data, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=BATCH_UPDATE)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=RandomSampler(val_dataset),\n",
    "    batch_size=BATCH_UPDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Hugging Face Trainer Class\n",
    "\n",
    "The provided code snippet utilizes the Hugging Face `Trainer` class for model training.\n",
    "\n",
    "- **Training Arguments (`training_args`):**\n",
    "  - Specifies training configurations such as output directory, number of epochs (`EPOCHS`), batch sizes (`TRAIN_BATCHSIZE`), gradient accumulation steps (`BATCH_UPDATE`), evaluation and save strategies, mixed-precision training (`fp16`), optimizer level (`APEX_OPT_LEVEL`), learning rate (`LR`), and other optimization parameters.\n",
    "\n",
    "- **Trainer Initialization (`trainer`):**\n",
    "  - Initializes the `Trainer` object with the specified model, training arguments (`args`), training dataset (`train_dataset`), evaluation dataset (`val_dataset`), and tokenizer (`tokenizer`).\n",
    "\n",
    "- **Training Execution (`trainer.train()`):**\n",
    "  - Executes the training process using the configured `Trainer` object (`trainer`), which includes model training based on the provided datasets and training arguments.\n",
    "\n",
    "- **Model Saving (`trainer.save_model()`):**\n",
    "  - Saves the trained model to the specified directory (`'/content/final_model_2'`) after training completion.\n",
    "\n",
    "The Hugging Face `Trainer` class encapsulates the entire training workflow, providing a high-level interface for efficient model training and management of training configurations and datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\pipelineEnv\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  3%|▎         | 26/892 [00:45<24:30,  1.70s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[62], line 31\u001b[0m\n",
      "\u001b[0;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n",
      "\u001b[0;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n",
      "\u001b[0;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,    \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     27\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n",
      "\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#---------------------------------------------------#   \u001b[39;00m\n",
      "\u001b[1;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/PersonalSpace/College/FYP/Code/new/models/final_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\pipelineEnv\\lib\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\pipelineEnv\\lib\\site-packages\\transformers\\trainer.py:2120\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n",
      "\u001b[0;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n",
      "\u001b[1;32m-> 2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[0;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n",
      "\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n",
      "\u001b[0;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n",
      "\u001b[0;32m   2124\u001b[0m ):\n",
      "\u001b[0;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\u001b[0;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "\u001b[0;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
    "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    fp16=True,\n",
    "    fp16_opt_level=APEX_OPT_LEVEL,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    learning_rate=LR,\n",
    "    adam_epsilon=EPS,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer.train()\n",
    "trainer.save_model('/content/final_model_2')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
