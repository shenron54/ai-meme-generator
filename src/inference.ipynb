{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\pipelineEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model loading \n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "classification_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "\n",
    "def get_tokenier(special_tokens=None):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\") #GPT2Tokenizer\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "\n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = GPT2Config.from_pretrained(\"gpt2\", \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = GPT2Config.from_pretrained(\"gpt2\",                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.from_pretrained(load_model_path)\n",
    "\n",
    "    model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import torch \n",
    "\n",
    "class_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",\n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "\n",
    "MAXLEN          = 128  #{768, 1024, 1280, 1600}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                  load_model_path='../models/final_model_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.load_state_dict(torch.load('../models/finetuned_BERT2_epoch_5.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\user\\.conda\\envs\\pipelineEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Steve Jobs Says\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text):\n",
    "    encoded_input = class_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True, \n",
    "        max_length=256, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = classification_model(**encoded_input)\n",
    "\n",
    "    logits = output.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()  # Get the predicted class for the single input sequence\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"AI models are great for meme generation.\"\n",
    "predicted_class = classify_text(input_sentence)\n",
    "predicted_class = list(label_dict.keys())[predicted_class]\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-p (Nucleus) Text Generation (10 Samples)\n",
    "\n",
    "The provided code snippet demonstrates text generation using the GPT-2 model with the top-p (nucleus) sampling strategy to generate 10 text samples (`num_return_sequences=10`).\n",
    "\n",
    "- **Model Generation (`model.generate()`):**\n",
    "  - Generates text based on the input `generated` prompt using the specified model.\n",
    "  - Utilizes top-p (nucleus) sampling (`do_sample=True`) to select tokens based on the cumulative probability distribution, controlling diversity and avoiding repetition in generated text.\n",
    "  - Specifies minimum (`min_length`) and maximum (`max_length`) length constraints for generated text.\n",
    "  - Configures parameters such as `top_k` (top k tokens to consider), `top_p` (cumulative probability threshold for nucleus sampling), `temperature` (sampling temperature), and `repetition_penalty` (penalization factor for token repetition).\n",
    "\n",
    "- **Text Decoding and Output:**\n",
    "  - Decodes the generated token sequences into readable text using the tokenizer (`tokenizer.decode()`), skipping special tokens.\n",
    "  - Computes the length of the predicted class and joined keywords to adjust the starting point (`a`) for displaying generated text.\n",
    "  - Prints each generated text sample (`text[a:]`) along with its corresponding index (`i+1`).\n",
    "\n",
    "This text generation process leverages advanced sampling techniques and model settings to generate diverse and contextually relevant text outputs based on the provided input prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "rake = Rake()\n",
    "\n",
    "def get_keywords(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom caption dataset \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, randomize=False):\n",
    "        template, caption, keywords  = [], [], []\n",
    "        for temp, cap, kws in data:\n",
    "            template.append(temp)\n",
    "            caption.append(cap)\n",
    "            keywords.append(kws)\n",
    "        \n",
    "        self.randomize = randomize    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.template = template\n",
    "        self.keywords = keywords\n",
    "        self.caption = caption\n",
    "    \n",
    "    @staticmethod\n",
    "    def join_keywords(keywords, randomize=False):\n",
    "        N = len(keywords)\n",
    "\n",
    "        #random sampling and shuffle\n",
    "        if randomize: \n",
    "            M = random.choice(range(N+1))\n",
    "            keywords = keywords[:M]\n",
    "            random.shuffle(keywords)\n",
    "\n",
    "        return ','.join(keywords)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        keywords = self.keywords[i].copy()\n",
    "        kw = self.join_keywords(keywords)\n",
    "        input = SPECIAL_TOKENS['bos_token'] + self.template[i] + \\\n",
    "                SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
    "                self.caption[i] + SPECIAL_TOKENS['eos_token']\n",
    "        \n",
    "        encodings_dict = self.tokenizer(input,\n",
    "                                   truncation = True,\n",
    "                                   max_length = MAX_LENGTH,\n",
    "                                   padding=\"max_length\")\n",
    "        \n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            \"label\": torch.tensor(input_ids),\n",
    "            \"input_ids\": torch.tensor(input_ids),            \n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAVE TO DO MEME GENERATION <sep> AI MODELS ARE GREAT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "keywords = get_keywords(input_sentence)\n",
    "kw = CaptionDataset.join_keywords(keywords, randomize=False)\n",
    "\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + predicted_class + \\\n",
    "         SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model = \"../models/final_model_2/\")\n",
    "output = generator(prompt)[0]['generated_text']\n",
    "\n",
    "# Extract the desired output\n",
    "desired_output = output.split('<|SEP|>')[2].strip()\n",
    "predicted_class = predicted_class.lower().replace(\" \", \"-\")\n",
    "print(desired_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAVE TO DO MEME GENERATION\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def textsize(text, font):\n",
    "    im = Image.new(mode=\"P\", size=(0, 0))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    _, _, width, height = draw.textbbox((0, 0), text=text, font=font)\n",
    "    return width, height\n",
    "\n",
    "# Load the image\n",
    "image_file = f\"../memes900k/images/{predicted_class}.jpg\"\n",
    "image = Image.open(image_file)\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Set the font and size for the text\n",
    "font = ImageFont.truetype(\"impact.ttf\", size=30)\n",
    "\n",
    "# Define text and calculate sizes\n",
    "text_parts = desired_output.split(\"<sep>\")\n",
    "top_text = text_parts[0].strip()\n",
    "bottom_text = text_parts[1].strip()\n",
    "padding = 3  # Padding around the text for the background\n",
    "print(top_text)\n",
    "\n",
    "top_text_width, top_text_height = textsize(top_text, font)\n",
    "bottom_text_width, bottom_text_height = textsize(bottom_text, font)\n",
    "\n",
    "lines = []\n",
    "\n",
    "# Handle the top text\n",
    "if top_text_width > image.width:\n",
    "    current_line = \"\"\n",
    "    for word in top_text.split():\n",
    "        if textsize(current_line + word, font)[0] <= image.width:\n",
    "            current_line += word + \" \"\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word + \" \"\n",
    "    lines.append(current_line.strip())\n",
    "    top_text_height = len(lines) * top_text_height\n",
    "    top_x =  padding\n",
    "else:\n",
    "    top_x = (image.width - top_text_width) / 2 - padding\n",
    "\n",
    "\n",
    "top_y = 5 - padding\n",
    "# Draw the background rectangle for each line of the bottom text\n",
    "y = top_y\n",
    "for line in lines:\n",
    "    line_width, line_height = textsize(line, font)\n",
    "    draw.rectangle([top_x, y, top_x + line_width + 2 * padding, y + line_height + 2 * padding], fill=\"black\")\n",
    "    draw.text((top_x + padding, y + padding), line, font=font, fill=(255, 255, 255))\n",
    "    y += line_height + padding\n",
    "\n",
    "lines = []\n",
    "\n",
    "# Handle the bottom text\n",
    "if bottom_text_width > image.width:\n",
    "    current_line = \"\"\n",
    "    for word in bottom_text.split():\n",
    "        if textsize(current_line + word, font)[0] <= image.width:\n",
    "            current_line += word + \" \"\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word + \" \"\n",
    "    lines.append(current_line.strip())\n",
    "    bottom_text_height = len(lines) * bottom_text_height\n",
    "    bottom_x = padding\n",
    "else:\n",
    "    bottom_x = padding\n",
    "\n",
    "bottom_y = image.height - bottom_text_height - 20 - padding\n",
    "\n",
    "\n",
    "# Draw the background rectangle for each line of the bottom text\n",
    "y = bottom_y\n",
    "for line in lines:\n",
    "    line_width, line_height = textsize(line, font)\n",
    "    draw.rectangle([bottom_x, y, bottom_x + line_width + 2 * padding, y + line_height + 2 * padding], fill=\"black\")\n",
    "    draw.text((bottom_x + padding, y + padding), line, font=font, fill=(255, 255, 255))\n",
    "    y += line_height + padding\n",
    "\n",
    "# Save the image\n",
    "image.save(f\"../generated_memes/{predicted_class}.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipelineEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
