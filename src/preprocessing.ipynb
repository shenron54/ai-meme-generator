{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "We use pandas to load the caption data from a text file. The file is tab-separated, and it doesn't contain a header row. Therefore, we explicitly specify the column names as \"Template\", \"val\", and \"Caption\" when loading the data into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../memes900k/captions.txt', sep='\\t', header=None, names=[\"Template\", \"val\", \"Caption\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data Distribution Among Templates\n",
    "\n",
    "We examine the distribution of data across different meme templates to identify any inconsistencies that may affect the analysis. This is done by counting the occurrences of each template within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Template\n",
       "Y U No                                                                      3000\n",
       "Buddy the Elf                                                               3000\n",
       "Office Space Boss                                                           3000\n",
       "Uncle Dolan                                                                 3000\n",
       "Pepperidge Farm Remembers FG                                                3000\n",
       "                                                                            ... \n",
       "skyrim stan                                                                 2964\n",
       "Grumpy Cat                                                                  2960\n",
       "Art Student Owl                                                             2959\n",
       "Chronic Illness Cat                                                         2952\n",
       "You keep using that word, I don't think it means what you think it means    2916\n",
       "Name: count, Length: 300, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Template\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the Labels\n",
    "\n",
    "For the classification model to process our data, we need to convert the categorical labels into numerical form. This is achieved by creating a dictionary that maps each unique template label to a unique index. This dictionary will be used to encode the labels in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the labels \n",
    "possible_labels = df.Template.unique()\n",
    "\n",
    "label_dict = {} \n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Label Encoding\n",
    "\n",
    "We apply the previously created label dictionary to transform the 'Template' column into a new 'label' column. This column contains numerical values corresponding to each template, facilitating the use of these labels in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'label_dict' (dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2764\\1522996728.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df.Template.replace(label_dict)\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df.Template.replace(label_dict)\n",
    "\n",
    "%store label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Split\n",
    "\n",
    "To prepare our data for model training and evaluation, we split it into training and validation sets using Scikit-learn's `train_test_split` function. We allocate 20% of the data to the validation set. The split is stratified by the label column to ensure that both sets have a representative distribution of each class.\n",
    "\n",
    "After the split, we add a new column `data_type` to the DataFrame, initially setting all values to 'not_set'. We then update this column to 'train' or 'val' based on the indices from our split, indicating whether each entry belongs to the training set or the validation set.\n",
    "\n",
    "Finally, we remove the 'val' column from the DataFrame, which is no longer needed, and display the count of entries for each combination of template, label, and data type, to verify our splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Template</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1889 [10] guy</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">102</th>\n",
       "      <th>train</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Advice Dog</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">76</th>\n",
       "      <th>train</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advice Hitler</th>\n",
       "      <th>110</th>\n",
       "      <th>train</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you have no power here</th>\n",
       "      <th>256</th>\n",
       "      <th>val</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">you mean to tell me black kid</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">196</th>\n",
       "      <th>train</th>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">your country needs you</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">167</th>\n",
       "      <th>train</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Caption\n",
       "Template                      label data_type         \n",
       "1889 [10] guy                 102   train         2400\n",
       "                                    val            600\n",
       "Advice Dog                    76    train         2400\n",
       "                                    val            600\n",
       "Advice Hitler                 110   train         2400\n",
       "...                                                ...\n",
       "you have no power here        256   val            600\n",
       "you mean to tell me black kid 196   train         2399\n",
       "                                    val            600\n",
       "your country needs you        167   train         2400\n",
       "                                    val            600\n",
       "\n",
       "[600 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and validation split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.20, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df = df.drop('val', axis=1)\n",
    "df.groupby(['Template', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "\n",
    "To optimize the training time and manage computational resources, we reduce the dataset size by sampling. Each 'Template' group in the DataFrame is halved by selecting 50% of its entries. This sampling is performed with a consistent random seed to ensure reproducibility.\n",
    "\n",
    "The process involves:\n",
    "- Grouping the DataFrame by 'Template' and applying a function to sample 50% of the data from each group.\n",
    "- Resetting the index of the sampled DataFrame to maintain order and consistency.\n",
    "- Displaying the count of entries for each combination of 'Template', 'label', and 'data_type' to verify the distribution in the sampled dataset.\n",
    "\n",
    "This approach balances the trade-off between computational efficiency and the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2764\\3342127304.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_data = df.groupby('Template').apply(lambda x: x.sample(frac=0.5, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'sampled_data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group the dataframe by 'Template' and sample 10% of the data for each group\n",
    "sampled_data = df.groupby('Template').apply(lambda x: x.sample(frac=0.5, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Reset the index of the sampled dataframe\n",
    "sampled_data = sampled_data.reset_index(drop=True)\n",
    "\n",
    "# Print the sampled dataframe\n",
    "sampled_data.groupby(['Template', 'label', 'data_type']).count()\n",
    "\n",
    "%store sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: Splitting Data into Train and Validation Sets\n",
    "\n",
    "To organize our data for training and validation, we create lists `train_data` and `val_data` to store captions and labels based on their respective data types ('train' or 'val').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 719473 [('Y U No', 'commercial <sep> y u no same volume as show!?'), ('Y U No', 'Victoria <sep> y u no tell us your secret?!'), ('Y U No', 'KONY <sep> Y u no take justin bieber'), ('Y U No', 'TED <sep> y u no tell us how you met their mother'), ('Y U No', 'universal remote <sep> y u no work on universe?')]\n",
      "Val data length: 179869 [('Y U No', 'Google <sep> Y U NO LET ME FINISH TYPING?'), ('Y U No', 'i held the door <sep> y u no say thank you'), ('Y U No', 'Team rocket <sep> y u no catch a different pikachu?'), ('Y U No', 'Y u no guy <sep> y u sound asian in my head?'), ('Y U No', 'hands <sep> y u no have same amount of fingers?')]\n"
     ]
    }
   ],
   "source": [
    "# Create lists to store captions and labels\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    template = row['Template']\n",
    "    caption = row['Caption']\n",
    "    data_type = row['data_type']\n",
    "    \n",
    "    # Append the template and caption data to the appropriate list based on data_type\n",
    "    if data_type == 'train':\n",
    "        train_data.append((template, caption))\n",
    "    elif data_type == 'val':\n",
    "        val_data.append((template, caption))\n",
    "\n",
    "# Print the length of the train and val data\n",
    "print(\"Train data length:\", len(train_data), train_data[:5])\n",
    "print(\"Val data length:\", len(val_data), val_data[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "rake = Rake()\n",
    "\n",
    "def get_keywords(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection\n",
    "\n",
    "In this section, we define a process to iterate over the dataset in order to find anomalies that causes the data to be misinterpreted due to incorrect separators. This affects the quality of the models we train, hence cleaning the dataset to a right amount is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 719473 719473 719473\n",
      "Val data length: 179869 179869 179869\n",
      "The top 10 strings with the largest number of words have 257 words each:\n",
      "1. COMPREI CAPINHAS NOVAS PRO MEU IPHONE 5 <sep> UI, ELA TEM IPHONE 5\n",
      "aysi\t0\tI'M A 10 PLAYA <sep> A 10\n",
      "aysi\t0\t10... <sep> THAT IS ALL\n",
      "aysi\t0\ti ain't one to gossip <sep> so you didn't hear it from me\n",
      "aysi\t0\tsly remarks at mods <sep> watch out we got a badass over here\n",
      "aysi\t0\tI HON DEINA KORTN ET GSEGN! <sep> <emp>\n",
      "aysi\t0\tHAY SI HAY SI <sep> SOY EL GERENTE PLANTHOM\n",
      "aysi\t0\tFOUND IT!!! <sep> <emp>\n",
      "aysi\t0\tOKAY, OKAY I AM <sep> INASINT\n",
      "aysi\t0\tSTAND BACK <sep> YOUR BLACK\n",
      "aysi\t0\t12 YEAR OLDS SMOKING WEED <sep> WE GOT SOME BADASS IS OVER HERE\n",
      "aysi\t0\tAY SI! AY SI! LE VOY A LOS LONGHORNS! <sep> Y NI ACABE LA HIGH SCHOOL!\n",
      "aysi\t0\tWOOHHHHH <sep> BADASS BLACK BOY OVER HERE\n",
      "aysi\t0\tIT AINT MY FAULT <sep> YOU DONT READ YOUR BIBLE I DIDNT SAY FOLLOW ME YOU DID THAT\n",
      "aysi\t0\tAY SI AY SI ESTOY FELIZ... <sep> PORQUE TENGO BICICLETA NUEVA\n",
      "aysi\t0\tKI E STATO A LASCIARE GABRIELE D'ASOLO? <sep> IOOOOOOOOOO... NO :'D\n",
      "aysi\t0\twhoa dude, my bad <sep> fat kids kill themselves every day, what makes her any different\n",
      "aysi\t0\tEven I hate Windows 8 <sep> <emp>\n",
      "aysi\t0\tWatch out <sep> Nick Cauly\n",
      "aysi\t0\tWatch out <sep> its facebook official\n",
      "aysi\t0\tbitch i told you <sep> the glove doesnt fit\n",
      "aysi\t0\tthe face i make when they ask for help with <sep> special projects\"\n"
     ]
    }
   ],
   "source": [
    "# Data inspection \n",
    "train_captions = []\n",
    "train_classes = []\n",
    "train_keywords = []\n",
    "\n",
    "val_captions = []\n",
    "val_classes = []\n",
    "val_keywords = []\n",
    "\n",
    "# Iterate over the train_data list\n",
    "for template, caption in train_data:\n",
    "    train_captions.append(caption)\n",
    "    train_classes.append(template)\n",
    "    kw = get_keywords(caption)\n",
    "    train_keywords.append(kw)\n",
    "\n",
    "# Iterate over the val_data list\n",
    "for template, caption in val_data:\n",
    "    val_captions.append(caption)\n",
    "    val_classes.append(template)\n",
    "    kw = get_keywords(caption)\n",
    "    val_keywords.append(kw)\n",
    "\n",
    "# Print the length of the train and val data\n",
    "print(\"Train data length:\", len(train_captions), len(train_classes), len(train_keywords))\n",
    "print(\"Val data length:\", len(val_captions), len(val_classes), len(val_keywords))\n",
    "\n",
    "\n",
    "all_captions = train_captions + val_captions    \n",
    "largest_word_count = 0\n",
    "longest_strings = []\n",
    "smallest_word_count = float('inf')\n",
    "shortest_string = \"\"\n",
    "\n",
    "for string in all_captions:\n",
    "    word_count = len(string.split())\n",
    "    if word_count > largest_word_count:\n",
    "        largest_word_count = word_count\n",
    "        longest_strings = [string]\n",
    "    elif word_count == largest_word_count:\n",
    "        longest_strings.append(string)\n",
    "\n",
    "# Sort the longest strings by length\n",
    "longest_strings.sort(key=len, reverse=True)\n",
    "\n",
    "# Print the top 10 longest strings\n",
    "print(f\"The top 10 strings with the largest number of words have {largest_word_count} words each:\")\n",
    "for i, string in enumerate(longest_strings[:10]):\n",
    "    print(f\"{i+1}. {string}\")\n",
    "\n",
    "# print(f\"\\nThe string with the smallest number of words has {smallest_word_count} words:\")\n",
    "# print(shortest_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipelineEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
